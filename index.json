[
{
	"uri": "http://localhost:1313/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "AWS Transfer Family SFTP connectors now support VPC-based connectivity Many organizations rely on the Secure File Transfer Protocol (SFTP) as the industry standard for exchanging critical business data. Traditionally, securely connecting to private SFTP servers required custom infrastructure, manual scripting, or exposing endpoints to the public internet.\nToday, AWS Transfer Family SFTP connectors now support connectivity to remote SFTP servers through Amazon Virtual Private Cloud (Amazon VPC) environments. You can transfer files between Amazon Simple Storage Service (Amazon S3) and private or public SFTP servers while applying the security controls and network configurations already defined in your VPC. This capability helps you integrate data sources across on-premises environments, partner-hosted private servers, or internet-facing endpoints, with the operational simplicity of a fully managed Amazon Web Services (AWS) service.\nNew capabilities with SFTP connectors The following are the key enhancements:\nConnect to private SFTP servers – SFTP connectors can now reach endpoints that are only accessible within your AWS VPC connection. These include servers hosted in your VPC or a shared VPC, on-premises systems connected over AWS Direct Connect, and partner-hosted servers connected through VPN tunnels. Security and compliance – All file transfers are routed through the security controls already applied in your VPC, such as AWS Network Firewall or centralized ingress and egress inspection. Private SFTP servers remain private and don’t need to be exposed to the internet. You can also present static Elastic IP or bring your own IP (BYOIP) addresses to meet partner allowlist requirements. Performance and simplicity – By using your own network resources such as NAT gateways, AWS Direct Connect or VPN connections, connectors can take advantage of higher bandwidth capacity for large-scale transfers. You can configure connectors in minutes through the AWS Management Console, AWS Command Line Interface (AWS CLI), or AWS SDKs without building custom scripts or third-party tools. How VPC- based SFTP connections work SFTP connectors use Amazon VPC Lattice resources to establish secure connectivity through your VPC. Key constructs include a resource configuration and a resource gateway. The resource configuration represents the target SFTP server, which you specify using a private IP address or public DNS name. The resource gateway provides SFTP connector access to these configurations, enabling file transfers to flow through your VPC and its security controls.\nThe following architecture diagram illustrates how traffic flows between Amazon S3 and remote SFTP servers.\nAs shown in the architecture, traffic flows from Amazon S3 through the SFTP connector into your VPC. A resource gateway is the entry point that handles inbound connections from the connector to your VPC resources. Outbound traffic is routed through your configured egress path, using Amazon VPC NAT gateways with Elastic IPs for public servers or AWS Direct Connect and VPN connections for private servers. You can use existing IP addresses from your VPC CIDR range, simplifying partner server allowlists. Centralized firewalls in the VPC enforce security policies, and customer-owned NAT gateways provide higher bandwidth for large-scale transfers.\nWhen to use this feature With this capability, developers and IT administrators can simplify workflows while meeting security and compliance requirements across a range of scenarios:\nHybrid environments – Transfer files between Amazon S3 and on-premises SFTP servers using AWS Direct Connect or AWS Site-to-Site VPN, without exposing endpoints to the internet. Partner integrations – Connect with business partners’ SFTP servers that are only accessible through private VPN tunnels or shared VPCs. This avoids building custom scripts or managing third-party tools, reducing operational complexity. Regulated industries – Route file transfers through centralized firewalls and inspection points in VPCs to comply with financial services, government, or healthcare security requirements. High-throughput transfers – Use your own network configurations such as NAT gateways, AWS Direct Connect, or VPN connections with Elastic IP or BYOIP to handle large-scale, high-bandwidth transfers while retaining IP addresses already on partner allowlists. Unified file transfer solution – Standardize on Transfer Family for both internal and external SFTP connectivity, reducing fragmentation across file transfer tools. Start building with SFTP connectors To begin transferring files with SFTP connectors through my VPC environment, I follow these steps:\nFirst, I configure my VPC Lattice resources. In the Amazon VPC console, under PrivateLink and Lattice in the navigation pane, I choose Resource gateways, choose Create resource gateway to create one to act as the ingress point into my VPC.\nNext, under PrivateLink and Lattice in the navigation pane, I choose Resource configuration and choose Create resource configuration to create a resource configuration for my target SFTP server. Specify the private IP address or public DNS name, and the port (typically 22).\nThen, I configure AWS Identity and Access Management (IAM) permissions. I ensure that the IAM role used for connector creation has transfer:* permissions, and VPC Lattice permissions (vpc-lattice:CreateServiceNetworkResourceAssociation, vpc-lattice:GetResourceConfiguration, vpc-lattice:AssociateViaAWSService). I update the trust policy on the IAM role to specify transfer.amazonaws.com as a trusted principal. This enables AWS Transfer Family to assume the role when creating and managing my SFTP connectors.\nAfter that, I create an SFTP connector through the AWS Transfer Family console. I choose SFTP Connectors and then choose Create SFTP connector.\nIn the Connector configuration section, I select VPC Lattice as the egress type, then provide the Amazon Resource Name (ARN) of the Resource Configuration, Access role, and Connector credentials. Optionally, include a trusted host key for enhanced security, or override the default port if my SFTP server uses a nonstandard port.\nNext, I test the connection. On the Actions menu, I choose Test connection to confirm that the connector can reach the target SFTP server.\nFinally, after the connector status is ACTIVE, I can begin file operations with my remote SFTP server programmatically by calling Transfer Family APIs such as StartDirectoryListing, StartFileTransfer, StartRemoteDelete, or StartRemoteMove. All traffic is routed through my VPC using my configured resources such as NAT gateways, AWS Direct Connect, or VPN connections together with my IP addresses and security controls.\nFor the complete set of options and advanced workflows, refer to the AWS Transfer Family documentation.\nNow available SFTP connectors with VPC-based connectivity are now available in 21 AWS Regions. Check the AWS Services by Region for the latest supported AWS Regions. You can now securely connect AWS Transfer Family SFTP connectors to private, on-premises, or internet-facing servers using your own VPC resources such as NAT gateways, Elastic IPs, and network firewalls.\n— Betty\n"
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "AWS Weekly Roundup: Amazon Quick Suite, Amazon EC2, Amazon EKS, and more (October 13, 2025) This week I was at the inaugural AWS AI in Practice meetup from the AWS User Group UK. AI-assisted software development and agents were the focus of the evening! Next week I’ll be in Italy for Codemotion (Milan) and an AWS User Group meetup (Rome). My sessions there will be about AI agents and context engineering. I am also excited to try the new Amazon Quick Suite that brings AI-powered research, business intelligence, and automation capabilities into a single workspace.\nLast week’s launches Here are the launches that got my attention this week:\nAmazon Quick Suite – A new agentic teammate that quickly answers your questions at work and turns those insights into actions for you. Read more in Esra’s launch post. Amazon EC2 – General-purpose M8a instances powered by the 5th Generation AMD EPYC (codename Turin) processors and compute-optimized C8i and C8i-flex instances powered by custom Intel Xeon 6 processors are now available. Amazon EKS – EKS and EKS Distro now support Kubernetes version 1.34 with several improvements. AWS IAM Identity Center – AWS Key Management Service keys can now be used to encrypt identity data stored in IAM Identity Center organization instances. Amazon VPC Lattice – You can now configure the number of IPv4 addresses assigned to resource gateway elastic network interfaces (ENIs). The IPv4 addresses are used for network address translation and determine the maximum number of concurrent IPv4 connections to a resource Amazon Q Developer – Amazon Q Developer can help you get information about AWS product and service pricing, availability, and attributes, making it easier to select the right resources and estimate workload costs using natural language. More info in this blog post. Amazon RDS for Db2 – You can now perform native database-level backups, offering greater flexibility in database management and migration. AWS Service Quotas – Get notified of your quota usage with automatic quota management. Configure your preferred notifications channels, such as email, SMS, or Slack. Notifications are also available in AWS Health, and you can subscribe to related AWS Cloudtrail events for automation workflows. Amazon Connect – You can now programmatically enrich case data with the new case APIs to link related cases, add custom related items, and search across them. You can now also customize service level calculations to your specific needs. New capabilities that have just been introduced include copy and bulk edit of agent scheduling configuration and agent schedule adherence notifications. AWS Client VPN – Now supports MacOS Tahoe. Additional updates Here are some additional projects, blog posts, and news items that I found interesting:\nServerless ICYMI Q3 2025 – A quarterly recap of serverless news, in case you missed it. Best practices for migrating from Apache Airflow 2.x to Apache Airflow 3.x on Amazon MWAA – A guide to help get the benefit of the new release. Building self-managed RAG applications with Amazon EKS and Amazon S3 Vectors – A reference architecture for building and deploying a self-managed RAG application using open source tools such as Ray, Hugging Face, and LangChain. BBVA: Building a multi-region, multi-country global Data and ML Platform at scale – A six-part series of posts describing the journey to transform BBVA entire data analytics infrastructure with one of the largest and most complex cloud migrations in the banking sector. Customizing text content moderation with Amazon Nova – Fine-tune Amazon Nova for content moderation tasks tailored to your requirements using domain-specific training data and organization-specific moderation guidelines. Upcoming AWS events Check your calendars so that you can sign up for these upcoming events:\nAWS AI Agent Global Hackathon – This is your chance to dive deep into our powerful generative AI stack and create something truly awesome. From September 8th to October 20th, you have the opportunity to create AI agents using AWS suite of AI services, competing for over $45,000 in prizes and exclusive go-to-market opportunities. AWS Gen AI Lofts – You can learn AWS AI products and services with exclusive sessions, meet industry-leading experts, and have valuable networking opportunities with investors and peers. Register in your nearest city: Paris (October 7–21), London (Oct 13–21), and Tel Aviv (November 11–19). AWS Community Days – Join community-led conferences that feature technical discussions, workshops, and hands-on labs led by expert AWS users and industry leaders from around the world: Budapest (October 16). Join the AWS Builder Center to learn, build, and connect with builders in the AWS community. Browse here upcoming in-person events, developer-focused events, and events for startups.\nThat’s all for this week. Check back next Monday for another Weekly Roundup!\n– Danilo\n"
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Announcing Amazon Quick Suite: your agentic teammate for answering questions and taking action Today, we’re announcing Amazon Quick Suite, a new agentic teammate that quickly answers your questions at work and turns those insights into actions for you. Instead of switching between multiple applications to gather data, find important signals and trends, and complete manual tasks, Quick Suite brings AI-powered research, business intelligence, and automation capabilities into a single workspace. You can now analyze data through natural language queries, find critical information across enterprise and external sources in minutes, and automate processes from simple tasks to complex multi-department workflows.\nHere’s a look into Quick Suite.\nBusiness users often need to gather data across multiple applications—pulling customer details, checking performance metrics, reviewing internal product information, and performing competitive intelligence. This fragmented process often requires consultation with specialized teams to analyze advanced datasets, and in some cases, must be repeated regularly, reducing efficiency and leading to incomplete insights for decision-making.\nQuick Suite helps you overcome these challenges by combining agentic teammates for research, business intelligence, and automation into a unified digital workspace for your day-to-day work.\nIntegrated capabilities that power productivity Quick Suite includes the following integrated capabilities:\nResearch – Quick Research accelerates complex research by combining enterprise knowledge, premium third-party data, and data from the internet for more comprehensive insights. Business intelligence – Quick Sight provides AI-powered business intelligence capabilities that transform data into actionable insights through natural language queries and interactive visualizations, helping everyone make faster decisions and achieve better business outcomes. Automation – Quick Flows and Quick Automate help users and technical teams to automate any business process from simple, routine tasks to complex multi-department workflows, enabling faster execution and reducing manual work across the organization. Let’s dive into some of these key capabilities.\nQuick Index: Your unified knowledge foundation Quick Index creates a secure, searchable repository that consolidates documents, files, and application data to power AI-driven insights and responses across your organization.\nAs a foundational component of Quick Suite, Quick Index operates in the background to bring together all your data—from databases and data warehouses to documents and email. This creates a single, intelligent knowledge base that makes AI responses more accurate and reduces time spent searching for information.\nQuick Index automatically indexes and prepares any uploaded files or unstructured data you add to your Quick Suite, enabling efficient searching, sorting, and data access. For example, when you search for a specific project update, Quick Index instantly returns results from uploaded documents, meeting notes, project files, and reference materials—all from one unified search instead of checking different repositories and file systems.\nTo learn more, visit the Quick Index overview page.\nQuick Research: From complex business challenges to expert-level insights Quick Research is a powerful agent that conducts comprehensive research across your enterprise data and external sources to deliver contextualized, actionable insights in minutes or hours — work that previously could take longer.\nQuick Research systematically breaks down complex questions into organized research plans. Starting with a simple prompt, it automatically creates detailed research frameworks that outline the approach and data sources needed for comprehensive analysis.\nAfter Quick Research creates the plan, you can easily refine it through natural language conversations. When you are happy with the plan, it works in the background to gather information from multiple sources, using advanced reasoning to validate findings and provide thorough analysis with citations.\nQuick Research integrates with your enterprise data connected to Quick Suite, the unified knowledge foundation that connects to your dashboards, documents, databases, and external sources, including Amazon S3, Snowflake, Google Drive, and Microsoft SharePoint. Quick Research grounds key insights to original sources and reveals clear reasoning paths, helping you verify accuracy, understand the logic behind recommendations, and present findings with confidence. You can trace findings back to their original sources and validate conclusions through source citations. This makes it ideal for complex topics requiring in-depth analysis.\nTo learn more, visit the Quick Research overview page.\nQuick Sight: AI-powered business intelligence Quick Sight provides AI-powered business intelligence capabilities that transform data into actionable insights through natural language queries and interactive visualizations.\nYou can create dashboards and executive summaries using conversational prompts, reducing dashboard development time while making advanced analytics accessible without specialized skills.\nQuick Sight helps you ask questions about your data in natural language and receive instant visualizations, executive summaries, and insights. This generative AI integration provides you with answers from your dashboards and datasets without requiring technical expertise.\nUsing the scenarios capability, you can perform what-if analysis in natural language with step-by-step guidance, exploring complex business scenarios and finding answers faster than before.\nAdditionally, you can respond to insights with one-click actions by creating tickets, sending alerts, updating records, or triggering automated workflows directly from your dashboards without switching applications.\nTo learn more, visit Quick Sight overview page.\nQuick Flows: Automation for everyone With Quick Flows, any user can automate repetitive tasks by describing their workflow using natural language without requiring any technical knowledge. Quick Flows fetches information from internal and external sources, takes action in business applications, generates content, and handles process-specific requirements.\nStarting with straightforward business requirements, it creates a multi-step flow including input steps for gathering information, reasoning groups for AI-powered processing, and output steps for generating and presenting results.\nAfter the flow is configured, you can share it with a single click to your coworkers and other teams. To execute the flow, users can open it from the library or invoke it from chat, provide the necessary inputs, and then chat with the agent to refine the outputs and further customize the results.\nTo learn more, visit the Quick Flows overview page.\nQuick Automate: Enterprise-scale process automation Quick Automate helps technical teams build and deploy sophisticated automation for complex, multistep processes that span departments, systems, and third-party integrations. Using AI-powered natural language processing, Quick Automate transforms complex business processes into multi-agent workflows that can be created merely by describing what you want to automate or uploading process documentation.\nWhile Quick Flows handles straightforward workflows, Quick Automate is designed for comprehensive and complex business processes like customer onboarding, procurement automations, or compliance procedures that involve multiple approval steps, system integrations, and cross-departmental coordination. Quick Automate offers advanced orchestration capabilities with extensive monitoring, debugging, versioning, and deployment features.\nQuick Automate then generates a comprehensive automation plan with detailed steps and actions. You will find a UI agent that understands natural language instructions to autonomously navigate websites, complete form inputs, extract data, and produces structured outputs for downstream automation steps.\nAdditionally, you can define a custom agent, complete with instructions, knowledge, and tools, to complete process-specific tasks using the visual building experience – no code required.\nQuick Automate includes enterprise-grade features such as user role management and human-in-the-loop capabilities that route specific tasks to users or groups for review and approval before continuing workflows. The service provides comprehensive observability with real-time monitoring, success rate tracking, and audit trails for compliance and governance.\nTo learn more, visit the Quick Automate overview page.\nAdditional foundational capabilities Quick Suite includes other foundational capabilities that deliver seamless data organization and contextual AI interactions across your enterprise.\nSpaces – Spaces provide a straightforward way for every business user to add their own context by uploading files or connecting to specific datasets and repositories specific to their work or to a particular function. For example, you might create a space for quarterly planning that includes budget spreadsheets, market research reports, and strategic planning documents. Or you could set up a product launch space that connects to your project management system and customer feedback databases. Spaces can scale from personal use to enterprise-wide deployment while maintaining access permissions and seamless integration with Quick Suite capabilities.\nChat agents – Quick Suite includes insights agents that you can use to interact with your data and workflows through natural language. Quick Suite includes a built-in agent to answer questions across all of your data and custom chat agents that you can configure with specific expertise and business context. Custom chat agents can be tailored for particular departments or use cases—such as a sales agent connected to your product catalog data and pricing information stored in a space or a compliance agent configured with your regulatory requirements and actions to request approvals.\nAdditional things to know If you’re an existing Amazon QuickSight customer – Amazon QuickSight customers will be upgraded to Quick Suite, a unified digital workspace that includes all your existing QuickSight business intelligence capabilities (now called “Quick Sight”) plus new agentic AI capabilities. This is an interface and capability change—your data connectivity, user access, content, security controls, user permissions, and privacy settings remain exactly the same. No data is moved, migrated, or changed.\nQuick Suite offers per-user subscription-based pricing with consumption-based charges for the Quick Index and other optional features. You can find more detail on the Quick Suite pricing page.\nNow available Amazon Quick Suite gives you a set of agentic teammates that helps you get the answers you need using all your data and move instantly from answers to action so you can focus on high value activities that drive better business and customer outcomes.\nVisit the getting started page to start using Amazon Quick Suite today.\nHappy building — Esra and Donnie\n"
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "http://localhost:1313/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Duy Khanh\nPhone Number: 0389430144\nEmail: ldk11072003@gmail.com\nUniversity: Sai Gon University\nMajor: Information Technology\nClass: DCT1216\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 30/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Get familiar with system scaling concepts (Load Balancer, Auto Scaling) and know how to practically deploy on AWS. Grasp the process of deploying applications and databases using RDS, managing machine images (AMIs), and creating Launch Templates. Understand and apply CloudWatch to monitor resources, track logs, create metrics, alarms, and dashboards. Learn the basics of Route 53, DNS resolver, and integration with Microsoft AD in a cloud environment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice + Deploy Akaunting Instance + Create instance + Configure Networking + Deploy Prestashop E-Commerce + Secure Wordpress application + Create Snapshot + Migrate to a larger instance + Create alarms to notify administrators if a specific event occurs - Learn about Auto Scaling Group, AMIs, and Launch Template - Practice + Set up basic network infrastructure for FCJ Management application + VPC and Subnet + Internet Gateway + Route Table + Security Groups + Create EC2 Instance + Create DB Subnet Group for Amazon RDS + Create Amazon RDS Database Instance + Install data for Database + Deploy web server + Prepare data for Predictive scaling + Create Amazon Machine Images (AMIs) from EC2 + Create Launch Templates 06/10/2025 06/10/2025 cloudjourney.awsstudygroup.com 3 - Learn about Elastic Load Balancing (ELB) + Target Group + Application Load Balancer + Network Load Balancer + Classic Load Balancer + Gateway Load Balancer - Learn about Scaling solutions / techniques + Manual scaling + Scheduled scaling + Dynamic scaling + Predictive scaling - Practice: + Create Target Group + Create Application Load Balancer + Check Elastic Load Balancing (ELB) deployment results + Create Auto Scaling Group + Connect ASG with Load Balancer + Set up Size and Scaling Policies + Set up notifications - Practice Scaling solutions / techniques + Manual scaling + Scheduled scaling + Dynamic scaling + Predictive scaling 07/10/2025 07/10/2025 cloudjourney.awsstudygroup.com and AWS Study Group Youtube 4 - Learn about AWS CloudWatch + Features + Container Insights + Key benefits - Learn about CloudWatch Metrics, CloudWatch Logs, CloudWatch Alarms, CloudWatch Dashboards 08/10/2025 08/10/2025 cloudjourney.awsstudygroup.com 5 - Practice: + Deploy CloudFormation Stack + View Metrics + Interact with charts in CloudWatch + Perform Metrics searches + Perform math operations + Create dynamic labels + View CloudWatch Logs + Generate logs from an application, then query these logs in CloudWatch Logs Insights + Extract valuable data from logs and convert them into metrics using CloudWatch Metric Filters + Set up Alarm for Error Log Metric + Create a simple Dashboard to centrally manage Metrics and Alarms 09/10/2025 10/10/2025 cloudjourney.awsstudygroup.com 6 - Learn about Route 53 + Outbound Endpoints + Inbound Endpoints + Route 53 Resolver Rules - Learn about AWS Quick Starts, AWS CloudFormation, AWS Directory Service - Practice: + Create Key Pair, initialize CloudFormation Template, configure Security Group + Connect to RDGW + Deploy Microsoft AD + Configure DNS + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Create Route 53 Inbound Endpoints + Test results 10/10/2025 10/10/2025 cloudjourney.awsstudygroup.com Week 5 Achievements: Deploy and manage web applications on EC2/Lightsail, including Akaunting, WordPress, and Prestashop, along with security configuration, snapshots, and instance upgrades. Set up Auto Scaling Group, Launch Template, and AMI to scale the system, applying scaling methods: manual, scheduled, dynamic, predictive. Create and manage Load Balancers (ALB, NLB) connected to Auto Scaling Group to balance load and ensure system stability. Monitor the system using CloudWatch: Metrics, Logs, Alarms, Dashboards, and extract data from logs to track performance. Deploy database with Amazon RDS, set up DB Subnet Group, and connect EC2 with RDS for complete application operation. Learn and practice Route 53, deploy internal DNS, Microsoft AD, configure Resolver Rules, and check connections. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Understand the process of building AMI, Launch Template and deploying WordPress in a scalable environment. Use Lambda Function to optimize costs, automatically start/stop EC2 Instances according to demand. Get familiar with Grafana to monitor data and systems. Know how to manage resources using Tags, Resource Groups and apply access control using IAM. Learn about AWS Systems Manager to run remote commands and manage patches (Patch Manager). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: deploy Wordpress application with Auto Scaling Group to ensure the application\u0026rsquo;s scalability according to visitor demand + Prepare VPC and subnet, create Security Group for EC2, Database and initialize EC2, Database + Install wordpress on EC2 + Perform Autoscaling creation for wordpress Instance + Initialize AMI from Webserver Instance + Initialize Launch Template + Initialize Target Group + Initialize Load Balancer + Initialize Auto Scaling Group + Backup and restore database + Initialize Cloudfront for Web Server 13/10/2025 13/10/2025 cloudjourney.awsstudygroup.com 3 - Learn about Lambda function - Practice: use Lambda function to optimize costs for your system on AWS environment + Create VPC, Security Group, EC2 + Incoming Web-hooks slack + Create tag for instance + Create Role for Lambda + Create Lambda Function performing Stop instances function + Create Lambda Function performing Start instances function + Check results 14/10/2025 14/10/2025 cloudjourney.awsstudygroup.com 4 - Learn about Grafana - Practice: + Create VPC and Subnet, Security Group, Linux EC2 Instance + Create IAM User, IAM Role, assign IAM Role to EC2 Instance + Install Grafana + Perform EC2 connection using MobaXterm + Monitor with Grafana 15/10/2025 15/10/2025 cloudjourney.awsstudygroup.com and grafana.com/grafana 5 - Learn about resource management using Tag and Resource Groups - Practice: + Create EC2 Instance with tag + Add or remove tags on individual resources and on resource groups + Filter resources by tag + Create a Resource Group - Practice: manage access to EC2 Resource Tag service with AWS IAM + Create IAM User + Create IAM Policy + Create IAM Role + Switch Role + Check IAM Policy + Proceed to access EC2 console in AWS Region - Tokyo + Proceed to access EC2 console in AWS Region - North Virginia + Proceed to create EC2 instance when there are no and there are Tags satisfying conditions + Edit Resource Tag on EC2 Instance + Check policy 16/10/2025 16/10/2025 cloudjourney.awsstudygroup.com 6 - Learn about AWS Systems Manager - Practice: manage Patch and run commands on multiple servers with AWS System Manager + Create VPC, Subnet, EC2 Instance, IAM Role and assign IAM Role + Set up Patch Manager + Run Command 17/10/2025 17/10/2025 cloudjourney.awsstudygroup.com Week 6 Achievements: Deploy WordPress according to scalable model including: create VPC/Subnet/SG, install Webserver, create AMI, Launch Template, Target Group, Load Balancer and Auto Scaling Group; simultaneously backup – restore database and configure CloudFront. Build cost optimization automation solution using Lambda, including create Role, assign tag, create Stop/Start EC2 Lambda Function and check activation via Slack Webhook. Deploy Grafana to monitor the system, including create Linux EC2, IAM User/Role, install Grafana and connect to track data. Manage resources using Tag and Resource Groups, simultaneously apply IAM to control access by Tag and test in multiple Regions. Use AWS Systems Manager to manage patches using Patch Manager and run remote commands with Run Command. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Understand and practice AWS Systems Manager – Session Manager, including public/private EC2 connection, setting up Endpoints, and managing session logs. Grasp concepts and basic \u0026amp; advanced practices of AWS CloudFormation, including creating templates, creating stacks, StackSets, and Drift Detection. Know how to use AWS Cloud9 to work with CloudFormation and the AWS environment. Understand and set up AWS IAM Identity Center (AWS SSO) in AWS Organization: Users, Groups, Permission Sets, and access delegation. Apply Time-based Access Control and Customer Managed Policies to set up advanced Permission Sets for each user group in the Organization. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Session Manager - Practice: work with Amazon System Manager - Session Manager + Create VPC, Public subnet, Private subnet, security group, public Linux and private Windows servers + Create IAM Role + Create connection to Public EC2 server + Create connection to Private EC2 server + Enable DNS hostnames + Create ssm Endpoint + Create ssmmessages Endpoint + Create ec2messages Endpoint + Assign IAM role and restart EC2 instance + Manage session logs + Update IAM Role + Create S3 bucket and S3 Gateway endpoint + Monitor session logs + Check Session logs in S3 + Port Forwarding + Create IAM User with SSM connection permissions + Install and configure AWS CLI and Session Manager Plugin + Perform Portforwarding 20/10/2025 20/10/2025 cloudjourney.awsstudygroup.com 3 - Learn about AWS CloudFormation and AWS Cloud9 - Practice: create a CloudFormation template and a few basic features of CloudFormation, and how to validate templates + Create IAM User, IAM Role + Create Workspace + Create CloudFormation Template 21/10/2025 21/10/2025 cloudjourney.awsstudygroup.com, https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html, https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-template-resource-type-ref.html 4 - Learn about StackSets, Drift Detection, Simple Queue Service - Practice: Advanced CloudFormation + Create Lambda Function + Create Stack in CloudFormation + Connect EC2 Instance + Mappings and Stacksets + Change resource configuration with Drift 22/10/2025 22/10/2025 cloudjourney.awsstudygroup.com and https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html 5 - Practice: Set up Single Sign-On (Amazon SSO) for Organization + Enable AWS Organizations + Enable IAM Identity Center + Create Users and Groups in IAM Identity Center + Create Permission Sets + Provision Permission Sets + Check access based on user, group, and permission set + Deploy CloudFormation Template + Verify access permissions + Check Administrator permissions + Check ReadOnly permissions + Configure AWS CLI - Manually refresh credentials + Configure AWS CLI - Automatically refresh credentials 23/10/2025 23/10/2025 cloudjourney.awsstudygroup.com 6 - Practice: Set up Single Sign-On (Amazon SSO) for Organization + Use Time-based access control to provide temporary access to AWS accounts for Security Auditor + Create Permission Set with necessary access permissions for Security Auditor as inline policy with time-based access control conditions + Create Group for Security Auditors + Assign users to Security Auditors Group + Assign access to AWS Account for Security Auditors group with newly created permission set + Create CMP, create permission set by attaching created CMP, and finally provision permission set for AWS account + Create Customer Managed IAM Policy + Create permission Set with Customer Managed policy + Create Group and create user and add to Group + Assign Permission set to AWS Account and verify access 24/10/2025 24/10/2025 cloudjourney.awsstudygroup.com Week 7 Achievements: Master Session Manager in AWS Systems Manager: create public/private VPC environment, assign IAM Role, set up SSM, EC2Messages, SSMMessages endpoints, and successfully connect to both Public and Private EC2. Manage and track Session Logs: configure S3 bucket, S3 Gateway Endpoint, update IAM Role to log, and verify logs functioning correctly in S3. Perform Port Forwarding via SSM: create IAM User, configure AWS CLI + SSM Plugin, perform secure port forwarding without opening ports in Security Group. Learn and use AWS CloudFormation: create Cloud9 Workspace, create IAM User/Role, write CloudFormation template, and validate template validity. Deploy advanced CloudFormation: create Lambda Function, build Stack, set up StackSets, map resources, connect EC2, and check Drift Detection when changing configuration. Set up Single Sign-On with IAM Identity Center for the entire AWS Organization: create Users/Groups, Permission Sets, grant Administrator/ReadOnly permissions, and verify permissions by user/group. Configure AWS CLI with IAM Identity Center: test manual and automatic credential refresh, ensuring correct permission set recognition. Apply Time-based Access Control to grant temporary access to Security Auditor: create Permission Set with time-based conditions, categorize Security Auditor group, assign permissions, and verify correct operation. Create Customer Managed Policy (CMP) and combine CMP into Permission Set, assign to AWS Account, and check actual user access permissions. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Understand and practice identity management with IAM Identity Center and User/Group APIs. Grasp the mechanism of limiting permissions using IAM Permission Boundary and conditions when switching Roles (Role Switching). Get familiar with security services: Security Hub, AWS WAF, and security assessment standards. Learn about monitoring – logging – encryption services like CloudTrail, KMS, and Athena. Know how to use AWS Backup and SNS to build backup and notification sending processes. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: IAM Identity Center Identity Store APIs + Environment Setup + Test Setup + Use case Prerequisites + AWS IAM Identity Center User and Group API Operations + Create User and add to the group + Update User’s Group Membership + AWS IAM Identity Center User and Group Audit Operations - Translate Blog: + Announcing Amazon Quick Suite: your agentic teammate for answering questions and taking action + AWS Transfer Family SFTP connectors now support VPC-based connectivity + AWS Weekly Roundup: Amazon Quick Suite, Amazon EC2, Amazon EKS, and more (October 13, 2025) 27/10/2025 27/10/2025 https://cloudjourney.awsstudygroup.com/ and https://aws.amazon.com/vi/blogs/aws/reimagine-the-way-you-work-with-ai-agents-in-amazon-quick-suite/#:~:text=Today%2C%20we%E2%80%99re%20announcing%20Amazon%20Quick%20Suite%2C%20a%20new,and%20turns%20those%20insights%20into%20actions%20for%20you. and https://aws.amazon.com/vi/blogs/aws/aws-transfer-family-sftp-connectors-now-support-vpc-based-connectivity/ and https://aws.amazon.com/vi/blogs/aws/aws-weekly-roundup-amazon-quick-suite-amazon-ec2-amazon-eks-and-more-october-13-2025/ 3 - Learn about IAM Permission Boundary - Practice: Limit User Permissions with IAM Permission Boundary + Create Boundary Policy + Create Limited IAM User + Check if the user assigned permissions is restricted by Permission Boundary - Practice: Limit Role switching by Condition + Create IAM Group + Create IAM Users and check their permissions + Configure Role Condition + Create IAM Role with Administrative permissions + Configure Switch role for IAM User + Limit allowed time to Switch role + Limit allowed IP to Switch role 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about AWS Security Hub, security standards - Practice: Enable Security Hub via console - Practice: Check assessment by each set of standards - Learn about AWS Web Application Firewall - Practice: perform environment creation for workshop including creating S3 bucket and deploying a Sample Application + Create S3 bucket + Deploy an OWASP Juice Shop Application - Practice: Use AWS WAF + Deploy Web ACLs with managed rules + Create Custom Rule + Define WAF rule in JSON format. Complex logic needs to be defined using AND, OR, and NOT operators + Test new Rule + Log requests 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ and https://aws.amazon.com/vi/waf/ 5 - Learn about AWS Key Management Service, AWS CloudTrail, Amazon Athena - Practice: Manage Keys with Key Management Service (AWS KMS) + Create Policy, Role, Group, and User + Create KMS + Create Bucket and upload data to S3 + Create CloudTrail and log into CloudTrail + Create Amazon Athena and query data with Athena + Test and share encrypted data on S3 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ and https://docs.aws.amazon.com/kms/latest/developerguide/overview.html and https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html and https://docs.aws.amazon.com/athena/latest/ug/what-is.html 6 - Learn about AWS Backup, AWS Simple Notification Service - Practice: Deploy system backup plan with AWS Backup + Create S3 bucket + Use AWS Backup to create a backup plan for resources running on AWS + Create Backup Plan + Use AWS SNS (Simple Notification Service) to send notifications related to ongoing backup activities + Check backup activity 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ and https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html and https://aws.amazon.com/vi/sns/ Week 8 Achievements: Centrally manage users using IAM Identity Center: set up environment, create user/group, update permissions, and perform audit operations via API. Enhance access control using Permission Boundary and Role Condition: limit user permissions, limit switch role by time/IP, and test actual permission delegation. Deploy application layer security solutions: enable Security Hub, assess according to security standards; deploy AWS WAF with managed rules, custom rules, JSON rule logic, and log requests. Manage encryption, monitor, and analyze logs: create KMS key, configure CloudTrail, upload S3 data, query logs using Athena, and test encrypted data sharing. Set up system backup process with AWS Backup: create backup plan, configure backup resources, combine with SNS to send notifications, and check backup results. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Understand and practice connecting multiple VPCs using VPC Peering and Transit Gateway. Grasp the container application deployment process: build, push image to Amazon ECR, run with Docker, Docker Compose, and deploy using Amazon ECS. Get familiar with CI/CD systems using GitLab CI, GitHub Actions, CodeBuild, and CodePipeline. Manage and automate application deployment with CodeCommit – CodeDeploy – CodePipeline, combined with CloudWatch Events. Learn about and configure AWS Storage Gateway and Windows Multi-AZ storage platform (file system, file share, performance, quota, deduplication). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about VPC Peering - Practice: Link Virtual Private Clouds (VPC) with VPC Peering + Initialize CloudFormation Templates + Create Security Group and EC2 instance + Update Network ACL + Create Peering connection + Enable Cross-Peer DNS - Learn about AWS Transit Gateway - Practice: Centrally manage connections with AWS Transit Gateway + Create Key Pair and initialize CloudFormation Template + Create an AWS Transit Gateway to act as a central hub connecting VPCs + Configure Transit Gateway Attachments to connect VPCs with the created Transit Gateway + Create and configure Route Table for Transit Gateway to manage traffic flow between VPCs + Configure Route Table for each VPC to route traffic through Transit Gateway 03/11/2025 03/11/2025 https://cloudjourney.awsstudygroup.com/ and https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html and https://docs.aws.amazon.com/vpc/latest/tgw/what-is-transit-gateway.html 3 - Learn about Amazon ECR, Amazon ECS, Cloud Map - Practice: deploy application on Docker using AWS services like EC2, RDS, and Amazon ECR + Deploy a sample application on the personal computer itself to observe and evaluate + Configure VPC, create Security Group, configure Role for ECR, log in to Docker Hub + Create DB Subnet Group and launch RDS Instance + Launch EC2 Instance, install required application libraries, and add database for testing + Deploy on Docker Image + Deploy with Docker Compose + Push Docker Image to Amazon ECR + Push Docker Image to Docker Hub - Practice: Deploy Application to Amazon Elastic Container Service (Amazon ECS) + Use results from Practice: deploy application on Docker using AWS services like EC2, RDS, and Amazon ECR + Create CodeDeploy role and add Subnet + Configure NAT gateway, Route Table, Security Group + Create namespace in Cloud Map + Create ECS Cluster + Create ECS Task Definition for Frontend and Backend + Create Target Group and configure Application Load Balancer + Deploy Blue/Green and Service Scaling with Backend + Deploy Rolling with Frontend 04/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ and https://aws.amazon.com/vi/ecr/ and https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html and https://docs.amazonaws.cn/en_us/cloud-map/latest/dg/what-is-cloud-map.html 4 - Learn about AWS CodePipeline, AWS CodeBuild - Practice: Deploy Application with AWS CodePipeline + Deploy CI/CD with Gitlab • Fork and Edit Code • Install Gitlab and Runner • Edit and Add Role • Create Tag and Track Pipeline • Check Results + Deploy CI/CD with Github Action • Clone Github template • Create a new project and push code to Github • Create Access key and Secret key in AWS IAM • Check Results + Deploy CI/CD with CodeBuild • Fork github repository • Create CodeBuild Frontend • Create CodeBuild Backend • Create tag for Repository • Check results - Monitor application with Container Insights (CloudWatch) - Route logs with Firelens + Create S3 Bucket and IAM Role for Task + Deploy Service with new Task Definition + Check results 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ and https://aws.amazon.com/vi/codepipeline/ and https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html 5 - Learn about AWS CodeCommit, AWS CodeDeploy, AWS CloudWatch Events - Practice: Automate Application Deployment AWS CodePipeline + Use results from Practice: deploy application on Docker using AWS services like EC2, RDS, and Amazon ECR + Create S3 bucket, Git credentials, Git connection, Service role for CodeDeploy, IAM user, instance profile for Amazon EC2 instance, and assign role to EC2 instance + Install CodeDeploy Agent + AWS CodeCommit to push code from local machine + Build project with CodeBuild + Deployment with AWS CodeDeploy + Use AWS CodePipeline to create CI/CD process deploying application - Learn about AWS Storage Gateway - Practice: Use AWS Storage Gateway + Create Storage Gateway + Create File Share + Connect File shares on On-premise machine 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ and https://docs.aws.amazon.com/whitepapers/latest/introduction-devops-aws/cloudwatch-events.html and https://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html and https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html and https://docs.aws.amazon.com/storagegateway/ 6 - Practice: Set up shared data storage system for Windows infrastructure. + Create practice environment + Create an SSD Multi-AZ file system + Create an HDD Multi-AZ file system + Create file share + Check performance of STG326 - SAZ. + Monitor performance of STG326 - SAZ + Enable data deduplication + Enable shadow copies + Manage User Sessions and open files + Enable user storage quotas + Enable Continuously Available shares + Expand throughput capacity + Expand storage capacity 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Establish network connection between VPCs using VPC Peering and Transit Gateway: create CloudFormation, configure Security Group, NACL, Route Tables, Transit Gateway Attachments, and Cross-Peer DNS. Deploy containerized application: build and run application via Docker, Docker Compose; create RDS, EC2, ECR repo; push image to ECR and Docker Hub; deploy to ECS with Cloud Map, ALB, Blue/Green, and Rolling update. Build CI/CD process with GitLab, GitHub Actions, and AWS CodeBuild: configure Runner, Credential, create pipeline, build frontend/backend, and monitor results. Automate deployment using CodeCommit – CodeBuild – CodeDeploy – CodePipeline: create IAM Role, Agent, S3 bucket, Git connection; build and deploy automatically via pipeline; route logs using FireLens. Set up enterprise storage system: create Storage Gateway, File Share; configure Windows Multi-AZ file system; monitor performance, enable deduplication, shadow copies, manage sessions, and expand capacity. "
},
{
	"uri": "http://localhost:1313/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Typically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Researching and practicing the deployment of AWS core infrastructure services, including VPC network design, EC2 server management, and Site-to-Site VPN setup\nWeek 3: Practicing advanced VPN configuration and in-depth EC2 management, while deploying real-world applications on multiple platforms and applying IAM policies to secure resources\nWeek 4: Mastering storage and content delivery services (S3, CloudFront), relational database management (RDS), practicing within the Cloud9 environment, and rapid application deployment with Amazon Lightsail\nWeek 5: Building scalable and load-balanced systems (Auto Scaling, ELB), deploying web applications connected to RDS, establishing in-depth monitoring via CloudWatch, and managing advanced routing with Route 53\nWeek 6: Optimizing operations and costs through auto-scaling WordPress deployment, applying Lambda for resource management, monitoring systems with Grafana, and practicing advanced administration using Tags, IAM, and Systems Manager\nWeek 7: Managing secure connections via Systems Manager, automating infrastructure with advanced CloudFormation, and setting up a centralized identity system (IAM Identity Center) for the entire AWS Organization\nWeek 8: System security and governance through advanced identity management (IAM Identity Center, Permission Boundary), deploying application protection layers (Security Hub, WAF), and establishing encryption, log monitoring, and automated backup mechanisms (KMS, CloudTrail, Athena, AWS Backup)\nWeek 9: Multi-VPC networking (Peering, Transit Gateway), deploying Container applications on ECS, building comprehensive automated CI/CD pipelines (AWS Code Suite, GitLab/GitHub), and setting up Hybrid Cloud storage solutions\nWeek 10: Mastering Amazon DynamoDB from advanced design patterns to Serverless and AI integration, alongside practicing system cost optimization and automated deployment of the TravelBuddy application on Elastic Beanstalk\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.1-week1/",
	"title": "Worklog Week 1",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of the First Cloud Journey. Understand basic AWS services and how to use the AWS Management Console \u0026amp; AWS CLI. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Get to know the FCJ members - Read and take note of the rules and regulations at the internship unit 08/09/2025 08/09/2025 3 - Learn about AWS and its service categories + Compute + Storage + Networking + Database + \u0026hellip; 09/09/2025 09/09/2025 cloudjourney.awsstudygroup.com and AWS Study Group YouTube 4 - Create an AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install and configure AWS CLI + Use AWS CLI 10/09/2025 10/09/2025 cloudjourney.awsstudygroup.com and AWS Study Group YouTube 5 - Manage costs using AWS Budgets - Request support via AWS Support - Manage access permissions using AWS Identity and Access Management (IAM) 11/09/2025 11/09/2025 cloudjourney.awsstudygroup.com 6 - Practice: + Create Cost Budget + Create Usage Budget + Create RI Budget + Create IAM Group and IAM User + Create IAM Role and OperatorUser + Switch Role for OperatorUser 12/09/2025 12/09/2025 cloudjourney.awsstudygroup.com and AWS Study Group YouTube Week 1 Achievements: Understood what AWS is and the basic service categories:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the local machine, including:\nSetting up Access Key and Secret Key Configuring default Region Checking configuration and account information Set up and managed costs through AWS Budgets:\nCost Budget Usage Budget RI Budget Managed user access using AWS IAM:\nCreated IAM Group, IAM User, and IAM Role Assigned permissions and switched roles for OperatorUser "
},
{
	"uri": "http://localhost:1313/1-worklog/1.2-week2/",
	"title": "Worklog Week 2",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand and practice AWS core infrastructure services, including VPC, EC2, and Site-to-Site VPN. Learn how to design, configure, and secure a basic VPC network. Deploy and manage EC2 Instances within Public and Private Subnets. Grasp the concept of network connectivity between on-premises and AWS Cloud through Site-to-Site VPN. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Learn the basics of VPC: + Subnets + Route Table + Internet Gateway + NAT Gateway + Security Groups + Network Access Control Lists - Practice: Build a VPC environment by deploying essential AWS networking components (VPC, Subnets, Route Table, Internet Gateway, Security Groups, VPC Flow Logs) and design a secure, scalable network structure. 15/09/2025 15/09/2025 cloudjourney.awsstudygroup.com and AWS Study Group YouTube 3 - Learn the fundamentals of EC2: + Concept, features, and operation of EC2 + Instance types (General Purpose, Compute Optimized, Memory Optimized, etc.) + Concepts of AMI, EBS, Security Group, and Key Pair + Network, storage, and access configuration for EC2 Instances + Instance lifecycle management (Start, Stop, Reboot, Terminate) 16/09/2025 16/09/2025 cloudjourney.awsstudygroup.com 4 - Learn about VPC Reachability Analyzer, EC2 Instance Connect Endpoint, AWS Systems Manager Session Manager, and CloudWatch Monitoring - Practice: Deploy Amazon EC2 Instances + Create EC2 instances in Public and Private Subnets + Connect via SSH + Create Elastic IP + Create NAT Gateway + Configure Route Table for Private Subnet + Analyze connections using VPC Reachability Analyzer + Deploy EIC Endpoint 17/09/2025 18/09/2025 cloudjourney.awsstudygroup.com 5 - Learn about AWS Site-to-Site VPN: + Virtual Private Gateway (VPG) + Customer Gateway (CGW) 18/09/2025 18/09/2025 cloudjourney.awsstudygroup.com 6 - Practice: + Configure VPC for Site-to-Site VPN + Deploy EC2 Instance for Customer Gateway + Create Virtual Private Gateway + Create Customer Gateway + Establish VPN connection between VPC and on-premises environment + Configure Customer Gateway + Adjust AWS VPN Tunnel settings 19/09/2025 19/09/2025 cloudjourney.awsstudygroup.com Week 2 Achievements: Gained a solid understanding of VPC architecture and core components, including Subnet, Route Table, Internet Gateway, NAT Gateway, Security Group, and Network ACL. Built a basic, scalable VPC model with secure access between subnets. Mastered fundamental knowledge of Amazon EC2, including: Instance classification and AMI selection Configuration of storage (EBS), security (Security Group, Key Pair), and networking (Elastic IP, Route Table) Instance lifecycle management (launch, connect, stop, terminate) Practiced deploying EC2 Instances in Public and Private Subnets, connecting via SSH, and managing access through Instance Connect Endpoint. Learned how to use VPC Reachability Analyzer and CloudWatch for monitoring and analyzing network connections. Understood the concepts, components, and setup process of Site-to-Site VPN between on-premises and AWS Cloud. Successfully practiced creating a secure VPN connection between Customer Gateway and Virtual Private Gateway, extending the on-premises network to AWS Cloud. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.3-week3/",
	"title": "Worklog Week 3",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Understand and practice advanced configurations of VPN and EC2 on AWS. Learn how to set up advanced VPN with StrongSwan, Transit Gateway, and BGP Dynamic Routing to extend network connectivity between regions and systems. Master advanced EC2 management and operation: changing instance types, creating custom AMIs, and recovering access when losing a Key Pair. Deploy real-world applications (Node.js and User Management) on Amazon Linux 2 and Windows Server 2022. Manage access control and resource usage limits with IAM Policy to enhance system security. Tasks to Implement This Week: Day Task Start Date Completion Date Reference 2 - Study advanced VPN configuration basics: + StrongSwan replacement setup + Advanced security configuration + BGP Dynamic Routing setup + Advanced monitoring and troubleshooting + Production deployment checklist - Explore common VPN setup issues and solutions for different OS environments - Practice: Configure VPN using StrongSwan with Transit Gateway + Create Customer Gateway + Create Transit Gateway + Create VPN connection + Create Transit Gateway Attachment + Configure Route Tables for Transit Gateway and VPC + Configure Customer Gateway 22/09/2025 22/09/2025 cloudjourney.awsstudygroup.com 3 - Review EC2 fundamentals: + Concepts, features, and how EC2 works + Instance types (General Purpose, Compute Optimized, Memory Optimized, etc.) + AMI, EBS, Security Group, Key Pair + Network, storage, and access configurations + Lifecycle management (Start, Stop, Reboot, Terminate) - Practice: + Create VPC and Security Groups for Linux and Windows + Launch a Microsoft Windows Server 2022 instance + Connect from local PC to Windows Server 2022 instance + Launch an Amazon Linux 2 instance + Connect to Amazon Linux 2 instance using MobaXterm and PuTTY 23/09/2025 23/09/2025 cloudjourney.awsstudygroup.com 4 - Practice: Advanced work with Amazon EC2 and related components + Change EC2 instance type + Create EC2 Snapshot + Create custom AMI + Launch new instance from custom AMI + Recover access when Key Pair is lost (Linux - User Data, Windows - SSM) - Research: + Desktop Interface for EC2 Ubuntu 22.04 + EBS Snapshots Archive + AMI Sharing 24/09/2025 24/09/2025 cloudjourney.awsstudygroup.com 5 - Practice: Deploy a Node.js application on Amazon Linux 2 + Prepare LAMP Web Server + Verify LAMP Web Server + Configure database server security + Install phpMyAdmin + Deploy application on Linux instance - Practice: Deploy AWS User Management application on Amazon EC2 (Windows Server 2022) + Install XAMPP on Microsoft Windows Server 2022 Base instance + Deploy application on Windows Server 2022 instance 25/09/2025 26/09/2025 cloudjourney.awsstudygroup.com 6 - Learn about resource usage limitation with IAM service - Practice: + Allow EC2 usage in Singapore region only + Restrict EC2 usage by Instance Family and Instance Type + Restrict EBS Volume Storage Type + Restrict EC2 deletion by company IP address + Restrict EC2 deletion by time window 26/09/2025 26/09/2025 cloudjourney.awsstudygroup.com Week 3 Achievements: Gained deeper understanding of advanced VPN configuration and security, including StrongSwan, Transit Gateway, BGP Dynamic Routing, and monitoring/troubleshooting techniques. Successfully implemented VPN connections across multiple VPCs with flexible and reliable architectures. Strengthened knowledge and hands-on skills in EC2 for both Linux and Windows: Created, configured, and connected to EC2 instances. Managed EC2 resources (EBS, Security Group, AMI). Recovered access using User Data and SSM Session Manager. Managed custom AMIs, snapshots, and recovery processes. Deployed real-world applications (Node.js and User Management) on Amazon Linux 2 and Windows Server 2022. Applied IAM Policy to restrict resource usage, enhancing security and cost control. Improved cloud operations, deployment, and infrastructure management skills at an advanced practical level. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.4-week4/",
	"title": "Worklog Week 4",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Understand and practice AWS core storage and content delivery services including S3, CloudFront, Cloud9, RDS, and Lightsail. Learn how to deploy, manage, and secure data in Amazon S3 (including Website Hosting, Versioning, and Cross-Region Replication). Understand how to deploy static and dynamic web applications using S3 + CloudFront + RDS + EC2. Get familiar with the AWS Cloud9 cloud development environment and understand the AWS Well-Architected Framework for optimal system design. Learn how to quickly deploy open-source applications (WordPress, PrestaShop) using Amazon Lightsail. Tasks for this week: Day Task Start Date End Date Reference 2 - Study the basics of AWS Cloud9 and practice: + Create a Cloud9 instance + Use the command line + Work with text files + Use AWS CLI on AWS Cloud9 - Learn the basics of AWS S3 + Concept and purpose of Object Storage + Structure of Bucket and Object + Common Storage Classes + Manage access control and data sharing (Public Access, Bucket Policy) - Practice: + Create an S3 Bucket + Upload data to S3 Bucket + Enable Static Website Hosting + Configure Block Public Access + Configure public object access + Verify hosted Website 29/09/2025 29/09/2025 cloudjourney.awsstudygroup.com 3 - Study the basics of Amazon CloudFront + Understand CDN concepts and CloudFront’s role in content delivery + Basic structure: Origin, Distribution, Edge Location + How CloudFront accelerates S3 content delivery - Learn about S3 Versioning + Key benefits of Versioning + How Versioning works + Versioning states - Practice: + Block all public access to S3 + Create CloudFront Distribution serving an S3 Bucket + Test Amazon CloudFront + Enable Versioning for the Bucket and update content + Test Versioning on S3 and CloudFront 30/09/2025 30/09/2025 cloudjourney.awsstudygroup.com and aws.amazon.com/cloudfront 4 - Learn about Amazon S3 Cross-Region Replication (CRR): + Key benefits + How CRR works - Study the AWS Well-Architected Framework: + Concept and six pillars of AWS Well-Architected Framework + Well-Architected Lenses - Practice: + Move objects to a new Bucket + Replicate S3 objects to another Region - Study Best Practices \u0026amp; Troubleshooting for AWS S3: + Security + Performance + Cost optimization + Common issues and resolutions 01/10/2025 01/10/2025 cloudjourney.awsstudygroup.com and aws.amazon.com/architecture/well-architected 5 - Study the basics of Amazon RDS: + Introduction to Relational Database Service + Supported database engines (MySQL, PostgreSQL, MariaDB, etc.) + Basic structure: DB Instance, Subnet Group, Security Group - Practice: + Create VPC and Security Groups for EC2 and RDS + Create a DB Subnet Group + Launch an EC2 instance + Create an RDS database instance + Deploy an application + Perform Backup and Restore in Amazon RDS 02/10/2025 03/10/2025 cloudjourney.awsstudygroup.com 6 - Practice: Deploy open-source applications on Amazon Lightsail and configure resources: + Deploy a database on Lightsail + Deploy a WordPress server + Configure Ubuntu + Configure Networking + Configure WordPress + Deploy PrestaShop E-Commerce instance 03/10/2025 03/10/2025 cloudjourney.awsstudygroup.com Week 4 Achievements: Became familiar with AWS Cloud9, used the CLI, and managed resources through the cloud development environment. Gained a solid understanding of Amazon S3 — created and configured Buckets, hosted a static website, enabled Block Public Access and Versioning. Understood global content delivery with Amazon CloudFront and integrated it with S3 to accelerate and secure websites. Learned and practiced S3 Cross-Region Replication (CRR), applying best practices for security, performance, and cost optimization. Deployed and connected Amazon RDS with EC2 instances, performed database Backup and Restore successfully. Studied the AWS Well-Architected Framework and understood its six architectural pillars. Successfully deployed and configured WordPress and PrestaShop applications on Amazon Lightsail, mastering resource management and basic security. "
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AI Career Coach 1. Executive Summary The AI Career Coach platform is designed for job seekers and fresh graduates to optimize the application process and enhance job opportunities. The system operates as an intelligent virtual assistant, supporting from building profiles (CV), writing cover letters to interview practice. The platform leverages the power of AWS Serverless to ensure automatic scalability and Amazon Bedrock (Generative AI) for deep content personalization. The system ensures user personal information security through Amazon Cognito.\n2. Problem Statement Current Problem Job seekers currently spend too much time manually editing CVs for each application position. Writing cover letters is often stereotypical, lacking highlights. Additionally, providing candidates with an environment to practice professional knowledge and analyze industry insights regarding their own career as quickly as possible. Solution The platform uses a comprehensive AWS Serverless architecture to solve the above problems:\nAmazon S3 \u0026amp; CloudFront: Store and distribute the web interface (SPA) at high speed. Amazon Cognito: Manage identity and authenticate users securely. Amazon API Gateway \u0026amp; AWS Lambda (Java Spring Cloud Function): Handle business logic according to Microservices architecture (User, Resume, Cover Letter, Interview). Amazon DynamoDB: Store profile data, assessment tests, and industry information with low latency. Amazon Bedrock: The heart of the system, providing the ability to upgrade CVs, draft Cover Letters, and generate interview questions according to real-world contexts. The platform focuses deeply on the \u0026ldquo;AI Personalization\u0026rdquo; feature; users not only store CVs but are also supported by AI. Key features include: CV upgrading, AI-based cover letter writing based on JD, taking knowledge review quizzes, industry trend reports. Benefits and Return on Investment (ROI) Reduce a lot of time writing cover letters and editing CVs. Leverage AWS Free Tier for Lambda, DynamoDB, and Cognito; costs mainly come from Bedrock API calls. Estimated operating cost is about 3-5 USD/month for personal use or small scale (under 1000 AI requests/month). The biggest value lies not in direct cash but in shortening cover letter writing and CV editing. No initial hardware costs.\n3. Solution Architecture The platform applies a fully AWS Serverless architecture to optimize scalability and maintenance costs. The user interface is distributed globally through Amazon CloudFront and S3. The backend system uses Microservices architecture with AWS Lambda (Java Spring Cloud Function) to handle business logic and Amazon Bedrock to integrate artificial intelligence. Profile and assessment data is centrally managed by Amazon DynamoDB, ensuring high performance and security. AWS Services Used\nAmazon CloudFront \u0026amp; S3: Store and distribute Web interface (Next.js) with low latency. Amazon Cognito: Manage identity, sign-up/sign-in, and secure user authentication. Amazon API Gateway: REST API gateway managing traffic and routing requests to the correct Lambda Service. AWS Lambda: Handle business logic (4 services: User, Resume, Cover Letter, Interview) on Java Spring environment. Amazon DynamoDB: NoSQL database storing user information, resumes, and assessment history (5 tables). Amazon Bedrock: Provide foundation models (Claude 3 Haiku/Sonnet) to analyze and generate content. Component Design\nUser Interface (Frontend): Next.js Single Page Application (SPA) interacting with backend through RESTful APIs. Access Management: Amazon Cognito (User Pool) authenticates users and issues JWT Tokens for API requests. Central Processing: AWS Lambda executes Spring Cloud Function functions, connecting with Bedrock to handle intelligent tasks (creating quizzes, fixing CVs). Data Layer: DynamoDB uses Multi-table design to ensure clear data separation and fast retrieval. Artificial Intelligence: Amazon Bedrock receives context from Lambda, performs inference, and returns consultation results or text content. 4. Technical Implementation Implementation Phases The project is divided into 2 major phases, focusing on designing optimal Serverless architecture for Java and integrating Generative AI:\nResearch, Design, and Feasibility Assessment: Design AWS Serverless data flow diagrams, define Microservices separation strategy with Spring Cloud Function. Select suitable AI model (Claude 3 Haiku) based on accuracy and speed benchmarks. Use AWS Pricing Calculator to estimate costs for Lambda (Java runtime), DynamoDB (Read/Write capacity), and most importantly, Amazon Bedrock Token costs. Set up expected budget (AWS Budget) to ensure the project stays within Free Tier limits or lowest cost. (Month 2) Development, Optimization, and Operation: Build Lambda functions using Java (Spring Boot 3), configure DynamoDB Multi-table, and write Prompt Engineering for Bedrock to optimize output results for CV/Interview. Integrate Frontend (Next.js) with API Gateway and Cognito, perform Integration Test for the whole system before Go-live. (Month 3) Technical Requirements\nBackend Services (Java Serverless): Proficient in Java 17/21 and Spring Cloud Function to write code according to the functional model. Deep understanding of AWS Lambda SnapStart mechanism to optimize Java application startup time (reducing latency from seconds to milliseconds). Generative AI \u0026amp; Data: Advanced Prompt Engineering skills to control Claude 3 model on Amazon Bedrock to return accurate JSON format. Effective DynamoDB schema design (Partition Key/Sort Key) for assessment history and user profile queries. Infrastructure \u0026amp; Security: Use Amazon Cognito to manage User Pool and JWT authentication. Configure Amazon API Gateway to map requests and handle CORS for Frontend. Deploy Frontend (Next.js) to Amazon S3 and distribute via CloudFront. 5. Roadmap \u0026amp; Milestones Internship (Month 1–3): - Month 1: Learn AWS and upgrade hardware. - Month 2: Design and adjust architecture. - Month 3: Deploy, test, put into use. Post-deployment: Research further to expand more functions. 6. Budget Estimation Costs can be viewed on AWS Pricing Calculator Infrastructure Costs (Region: Asia Pacific - Singapore)\nAmazon Bedrock: 2.70 USD/month (AI model, processing ~1,000 token input/output per request). Amazon CloudFront: 0.85 USD/month (Data transfer to internet 10 GB). Amazon DynamoDB: 0.28 USD/month (Storage 1 GB, Standard mode). AWS Lambda: 0.13 USD/month (4,000 requests, 512 MB temporary memory). Amazon S3: 0.05 USD/month (Storage 1 GB, 4,000 PUT/GET requests). Total: 4.01 USD/month, equivalent to 48.12 USD/12 months. 7. Risk Assessment Risk Matrix\nAI Hallucination: High Impact, Medium Probability. (AI gives misleading advice or fabricates information in CV). Cost Overrun: Medium Impact, Low Probability. (Due to Bedrock token fees if request volume increases suddenly). System Latency (Cold Start): Medium Impact, Low Probability. (Due to the nature of Java Lambda functions when starting up). Mitigation Strategies\nAccurate AI: Optimize Prompt Engineering with specific context, set low Temperature parameters (0.1 - 0.2) to reduce randomness. Add disclaimer warnings for users. Cost Control: Set up AWS Budgets to alert when costs reach 80% of the allowed threshold. Performance: Enable AWS Lambda SnapStart feature to reduce Java startup time from seconds to milliseconds. Contingency Plan\nAI Service Incident: Design the system to \u0026ldquo;fail gracefully\u0026rdquo; (friendly error notification) or return available templates if Amazon Bedrock is interrupted. Data Recovery: Enable DynamoDB Point-in-Time Recovery (PITR) feature to restore user data in case of accidental deletion or application errors. 8. Expected Outcomes Technical Improvements: Automate 90% of the application profile preparation process (from editing CVs to writing cover letters), completely replacing time-consuming manual drafting. The system achieves low latency (milliseconds) thanks to Java SnapStart optimization, capable of handling high loads without infrastructure intervention. Long-term Value: Successfully build a sample architecture framework for Java Serverless applications combined with GenAI on AWS, reusable for enterprise projects. Create a real-world environment to test and refine complex Prompt Engineering techniques, serving the specialized development direction of AI Engineer.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Understand and comprehensively practice Amazon DynamoDB: create tables, read-write data, query, scan, index, backup/restore, and communicate via AWS CLI. Grasp advanced DynamoDB models such as: Global Secondary Index, Sparse Index, Write Sharding, Sequential \u0026amp; Parallel Scan, Streams \u0026amp; Lambda, Adjacency Lists, Composite Keys. Learn about serverless and event-driven solutions based on DynamoDB such as: Global Tables, DynamoDB Streams, Lambda Trigger, Zero-ETL Pipeline with OpenSearch \u0026amp; Bedrock. Master AWS Cost Optimization: Savings Plans, Reserved Instances, EC2 Rightsizing, Cost Explorer, Glue + Athena for advanced cost analysis. Practice deploying the TravelBuddy application to Elastic Beanstalk and configuring automated deployment via CodeStar, CodeCommit, CodeDeploy. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about DynamoDB - Practice: use DynamoDB + Launch CloudFormation stack + Create DynamoDB Tables + Load Sample Data - Practice: Explore DynamoDB with CLI and console + Read sample data + Read Item Collections using Query + Work with Table Scans + Add/Modify data + Delete Data + Transactions + Global Secondary Indexes - Practice: Backup with Amazon DynamoDB + DynamoDB Point-in-time recovery + On-demand backup + Scheduled backup + Restrict backup deletion - Practice: LMIG: Relational Modeling \u0026amp; Migration + Configure MySQL environment + Create DMS resources + Explore Source Model + Explore Target Model + Load DynamoDB Table + Access DynamoDB Table - Practice: LBED: Generative AI with DynamoDB zero-ETL integration into OpenSearch and Amazon Bedrock + Configure OpenSearch Service permissions + Enable Amazon Bedrock model + Load DynamoDB Data + Configure ML connections and Pipeline in OpenSearch Service + Create Zero-ETL Pipeline + Query and conclude 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ and https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html and https://www.youtube.com/watch?v=xfxBhvGpoa0 3 - Practice: LADV: Advanced Design Patterns for Amazon DynamoDB + Exercise 1: DynamoDB Capacity Units and Partitioning • Create DynamoDB table • Load sample data into table • Load larger file to compare execution time • View CloudWatch metrics on your table • Increase table capacity • After increasing table capacity, load more data • Create new table with low capacity global secondary index + Exercise 2: Sequential and Parallel Table Scans • Perform sequential scan • Perform parallel scan + Exercise 3: Global Secondary Index Write Sharding • Create GSI • Query GSI using shards + Exercise 4: Global Secondary Index Overloading • Create employees table for overloaded global secondary index key • Load data into new table • Query employees table using global secondary index with overloaded attributes + Exercise 5: Sparse Global Secondary Indexes • Add new global secondary index to employees table • Scan employees table to find managers without using sparse global secondary index • Scan employees table to find managers using sparse global secondary index - Exercise 6: Composite Keys + Create new global secondary index for City-Department + Query all employees from state + Query all employees of a city + Query all employees of a specific city and department - Exercise 7: Adjacency Lists + Create and load InvoiceandBilling table + Review InvoiceAndBills table on DynamoDB console + Query invoice details + Query customer and invoice details using Index - Exercise 8: Amazon DynamoDB Streams and Lambda + Create replica table + Review IAM policy for role + Create Lambda function + Enable DynamoDB Stream + Map source stream to Lambda + Populate logfile table and verify replication to logfile_replica - Practice: LCDC: Change Data Capture for Amazon DynamoDB Start • Cloud9 • EC2 Instance Scenario Overview • Create DynamoDB table • Load sample data Update IAM Role • Enable DynamoDB Stream • Create Dead Letter Queue • Create Lambda Function • Simulate order update Change CDC using Kinesis Data Streams • Enable Kinesis Data Stream • Create DLQ • Create Lambda • Configure Lambda • Simulate order update 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Practice: LMR: Build and deploy global serverless application with DynamoDB Start Explore Global Tables Interact with Globalflix UI Discuss Global Tables Summary - Practice: LEDA: Serverless Event Driven Architecture with DynamoDB Preparation Overview Lab 1: Pipeline Deep Dive • Connect pipeline • Connect StateLambda • Check MapLambda trigger • Connect ReduceLambda Lab 2: Fault Tolerance \u0026amp; Exactly-Once Processing • Prevent duplicates at StateLambda • Ensure idempotency at ReduceLambda Summary 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Savings Plan and Reserved Instances, CloudWatch - Practice: EC2 Rightsizing \u0026amp; Compute Optimizer + Create IAM Role for CloudWatch Agent + Attach IAM Role to EC2 instance + Install CloudWatch Agent Bucket + Update Resource Optimization + Best Practices for EC2 sizing + Optimize VM according to Compute Optimizer - Practice: Visualize AWS costs + View costs by service + View costs by account + View Savings Plan coverage + View elasticity + View Reserved Instance coverage + Create custom EC2 reports + Analyze costs using Cost Explorer + Overview of Data Transfer Out in common architectures - Learn about AWS Glue - Practice: Advanced cost analysis with Glue + Athena + Prepare, build DB + Explore data in table + Tagging \u0026amp; Cost Allocation + Savings Plan, RI, On-demand, Spot Usage 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ and https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html and https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html 6 - Practice: Deploy TravelBuddy application to AWS + Create Key Pair, CloudFormation stack + Connect Windows instance, install RDS + Test locally with Eclipse IDE + Deploy Elastic Beanstalk + Update application + Query API in Eclipse IDE - Practice: Configure automated application release + CloudFormation template + Create AWS CodeStar project + Connect Eclipse IDE with CodeCommit + Update source code and overwrite template page with TravelBuddy + Commit changes to trigger Elastic Beanstalk deployment + Use AWS CodeDeploy to push Windows Service to EC2 + Monitor service 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Learn DynamoDB from basic to advanced: create tables, load sample data, CRUD via CLI/Console, Query/Scan, Transactions, Item Collections, and Global Secondary Index. Deploy full DynamoDB backup mechanisms: PITR, backup on-demand, scheduled backup, and restrict backup deletion. Apply advanced design patterns: Capacity Units, Partitioning, Sequential/Parallel Scan, Write Sharding, handle overloaded GSI, Sparse Index, Composite Keys, and Adjacency Lists. Integrate DynamoDB Streams with Lambda to replicate data to replica table; configure IAM; create trigger and verify replication. Build Serverless \u0026amp; Event-Driven architecture: Global Tables, Globalflix lab, StateLambda → MapLambda → ReduceLambda pipeline with idempotency and deduplication; Zero-ETL DynamoDB – OpenSearch – Bedrock. Optimize AWS costs: Savings Plan, RI, Rightsizing, Compute Optimizer, Cost Explorer, analyze Data Transfer Out, Glue + Athena for advanced cost queries. Deploy TravelBuddy: set up environment, deploy Elastic Beanstalk, update, connect CI/CD (CodeStar → CodeCommit → CodeDeploy), push Windows Service, and monitor service. "
},
{
	"uri": "http://localhost:1313/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Below are the blog posts I have translated:\nBlog 1 - AWS Transfer Family SFTP connectors now support VPC-based connectivity This blog announces that AWS Transfer Family SFTP connectors now support VPC-based connectivity, enabling secure file transfers between Amazon S3 and private or public SFTP servers without exposing endpoints to the internet. By leveraging existing VPC resources like NAT gateways, VPNs, and firewalls, users can integrate on-premises systems and partner-hosted servers while maintaining compliance and performance. Configuration is streamlined via AWS Console or CLI, making it easier to manage hybrid environments and regulated workloads.\nBlog 2 - AWS Weekly Roundup: Amazon Quick Suite, Amazon EC2, Amazon EKS, and more (October 13, 2025) This blog highlights the latest AWS updates as of October 13, 2025, including the launch of Amazon Quick Suite—an AI-powered workspace for research and automation—and new EC2 instances using AMD and Intel processors. It also covers Kubernetes 1.34 support in Amazon EKS, enhanced encryption in IAM Identity Center, and improvements in Amazon VPC Lattice, Amazon RDS for Db2, and Amazon Connect. Additionally, it announces upcoming AWS events like the AI Agent Global Hackathon and Gen AI Lofts across Europe.\nBlog 3 - Announcing Amazon Quick Suite: your agentic teammate for answering questions and taking action This blog introduces Amazon Quick Suite, a new AI-powered digital workspace that acts as your agentic teammate to answer questions and automate tasks. It combines research, business intelligence, and automation tools—like Quick Research, Quick Sight, Quick Flows, and Quick Automate—into a unified platform. With natural language queries, users can analyze data, generate insights, and streamline workflows across departments. Features like Quick Index and custom chat agents further enhance productivity by centralizing knowledge and enabling contextual AI interactions across the enterprise.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "http://localhost:1313/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]